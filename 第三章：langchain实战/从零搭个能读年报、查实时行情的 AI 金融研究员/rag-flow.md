# Deep RAG Pipeline - Complete Workflow

---

## ğŸ¯ Overview

**Multi-Agent Deep RAG for Financial Documents**

A complete pipeline for extracting, embedding, and retrieving information from financial SEC filings (10-K, 10-Q, 8-K reports) using multimodal content and advanced retrieval strategies.

---

## ğŸ“Š Pipeline Architecture

```
PDFs â†’ Extract â†’ Describe â†’ Embed â†’ Retrieve â†’ Answer
  â†“       â†“         â†“         â†“        â†“
 Step 1  Step 2   Step 3   Step 4   Step 5
```

---

## Step 1ï¸âƒ£: PDF Extraction with Docling

### Input
- Financial PDFs (10-K, 10-Q, 8-K reports)
- Organized in: `data/rag-data/pdfs/`

### Process
1. **Parse PDFs** using Docling converter
2. **Extract three content types:**
   - ğŸ“„ **Markdown**: Full document text with page breaks
   - ğŸ“Š **Tables**: With 2 paragraphs of context + page numbers
   - ğŸ–¼ï¸ **Images**: Large charts/diagrams (>500x500 pixels)

### Output Structure
```
data/rag-data/
â”œâ”€â”€ markdown/{company}/{document}.md
â”œâ”€â”€ tables/{company}/{document}/table_X_page_Y.md
â””â”€â”€ images/{company}/{document}/page_Y.png
```

### Key Features
- Page-level tracking for all content
- Contextual table extraction
- Smart image filtering (size-based)
- Metadata extraction from filenames

---

## Step 2ï¸âƒ£: Image Description Generation

### Input
- Extracted images from Step 1
- Located in: `data/rag-data/images/`

### Process
1. **Load images** using PIL
2. **Encode to base64** for API transmission
3. **Generate descriptions** using Gemini 2.5 Flash Multimodal 
4. **Save as markdown** files

### AI Prompt Focus
- Chart/graph data trends and axis labels
- Table structures and key data points
- Text content summaries
- Visual layout descriptions

### Output
```
data/rag-data/images_desc/
â””â”€â”€ {company}/{document}/page_Y.md
```

### Why This Step?
**Unified Embedding Space:** Convert visual content to text so everything can use the same embedding model (Approach 2: Text-only embeddings)

---

## Step 3ï¸âƒ£: Vector Database Ingestion

### Input
- Markdown files (Step 1)
- Table files (Step 1)
- Image descriptions (Step 2)

### Process

#### 3.1 Initialize Components
```python
# Gemini Embeddings (Full dimensionality)
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001"
)

# BM25 Sparse Embeddings
sparse_embeddings = FastEmbedSparse(
    model_name="Qdrant/bm25"
)

# Vector Store (Hybrid Mode)
vector_store = QdrantVectorStore(
    retrieval_mode=RetrievalMode.HYBRID
)
```

#### 3.2 Content Processing

**For Each Content Type:**

| Content Type | Source | Chunking Strategy |
|--------------|--------|-------------------|
| **Text** | Markdown files | Split by `<!-- page break -->` markers |
| **Tables** | Table MD files | Individual table + context (no splitting) |
| **Images** | Description MD files | Full description (no splitting) |

#### 3.3 Metadata Enrichment

**Extracted Metadata:**
- `company_name` (e.g., "amazon", "apple")
- `doc_type` (e.g., "10-k", "10-q", "8-k")
- `fiscal_year` (e.g., 2024)
- `fiscal_quarter` (e.g., "q3")
- `content_type` ("text", "table", "image")
- `page` (page number)
- `file_hash` (for deduplication)

### Output
**Single Qdrant Collection**
- **Hybrid search enabled** (dense + sparse)
- **Rich metadata** for filtering
- **All content types unified** in one collection

---

## Step 4ï¸âƒ£: Advanced Retrieval

### Retrieval Pipeline

#### 4.1 Filter Extraction with LLM

**Natural Language â†’ Structured Filters**

```python
User Query: "Amazon Q3 2024 revenue"
    â†“
LLM Extraction
    â†“
Filters: {
    "company_name": "amazon",
    "doc_type": "10-q",
    "fiscal_year": 2024,
    "fiscal_quarter": "q3"
}
```

**Gemini 2.5 Flash** extracts structured metadata from conversational queries

#### 4.2 Hybrid Search

**Dense + Sparse Retrieval**

```
Query: "What is Apple's revenue?"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dense Search   â”‚  Sparse Search  â”‚
â”‚  (Semantic)     â”‚  (Keyword)      â”‚
â”‚  Gemini-001     â”‚  BM25           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
       Reciprocal Rank
           Fusion
               â†“
         Top K Results
```

**Why Hybrid?**
- **Dense**: Understands semantic meaning
- **Sparse**: Matches exact keywords
- **Combined**: Best of both worlds

#### 4.3 Reranking

**Cross-Encoder Reranking**

```
Initial Results (k=10)
    â†“
BAAI/bge-reranker-base
(Cross-Encoder)
    â†“
Reranked Results (top_k=5)
(Sorted by relevance score)
```

**Purpose:** Deep interaction between query and documents for precise ranking

---

## Step 5ï¸âƒ£: Complete Retrieval Flow

### Function: `retrieve_with_reranking()`

```
User Query
    â†“
1. Extract Filters (LLM)
    â†“
2. Hybrid Search (Dense + Sparse)
   â€¢ Apply metadata filters
   â€¢ Fetch top K candidates
    â†“
3. Rerank (Cross-Encoder)
   â€¢ Score query-document pairs
   â€¢ Return top N results
    â†“
Final Results
```

---

## ğŸ”‘ Key Design Decisions

### 1. Unified Text Embeddings (Approach 2)
**Decision:** Use text embeddings for ALL content types
- âœ… Single embedding model (Gemini-001)
- âœ… Unified search across all types
- âœ… Simple architecture
- âœ… Cost-effective

**Alternative Rejected:**
- âŒ Multimodal embeddings (API issues, complexity)

### 2. Hybrid Search
**Dense + Sparse = Better Results**
- Dense: "revenue growth" â†’ finds "increased sales"
- Sparse: "Q3 2024" â†’ exact match
- Together: Comprehensive retrieval

### 3. LangChain Abstractions
**Simplified Code:**
```python
# Before (Raw Qdrant)
embedding = embeddings.embed_query(text)
sparse = sparse_embeddings.embed_query(text)
qdrant_client.upsert(points=[...])

# After (LangChain)
doc = Document(page_content=text, metadata=metadata)
vector_store.add_documents([doc])
```

### 4. Metadata-Driven Filtering
**Structured Filters > Text Search**
- Company, year, quarter â†’ precise filtering
- LLM extracts filters from natural language
- Reduces search space before semantic retrieval

---

## ğŸ“ˆ Performance Characteristics

### Scalability
- Handles documents of any size
- Automatic chunking by pages
- Deduplication prevents redundancy
- Incremental ingestion supported

### Retrieval Speed
- **Hybrid Search:** Fast vector + keyword matching
- **Reranking:** Batch processing for efficiency
- **End-to-end:** Sub-second response times

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **PDF Extraction** | Docling | Convert PDFs to structured content |
| **Vision** | Gemini 2.5 Flash | Generate image descriptions |
| **Embeddings** | Gemini Embedding 001 | Dense semantic vectors (full dimensionality) |
| **Sparse** | FastEmbed BM25 | Keyword matching |
| **Vector DB** | Qdrant | Hybrid search storage |
| **Framework** | LangChain | Abstraction layer |
| **Reranker** | BAAI/bge-reranker-base | Cross-encoder reranking |
| **LLM** | Gemini 2.5 Flash | Filter extraction, Q&A |

---

## ğŸ“ Learning Outcomes

### For Students
1. **End-to-end RAG pipeline** from PDFs to answers
2. **Multimodal content handling** (text, tables, images)
3. **Advanced retrieval strategies** (hybrid + reranking)
4. **Production-ready patterns** (deduplication, metadata)
5. **LangChain best practices** for clean code

---

## ğŸš€ Future Enhancements

### Potential Improvements
1. **Multi-query retrieval** - Generate multiple query variations
2. **Contextual compression** - Filter irrelevant context
3. **Parent-child retrieval** - Link chunks to full documents
4. **Graph-based RAG** - Entity relationships
5. **Streaming responses** - Real-time answer generation

---

## ğŸ“ Summary

### Progressive Flow
```
PDFs (unstructured)
    â†“
Extraction (structured: text + tables + images)
    â†“
Vision AI (images â†’ text descriptions)
    â†“
Embeddings (all text â†’ vectors)
    â†“
Vector DB (hybrid storage)
    â†“
Retrieval (filters + hybrid search + reranking)
    â†“
Answers (precise, relevant, sourced)
```

### Core Principle
**"Everything is Text, Everything is Searchable"**

By converting all modalities to text and using unified embeddings with hybrid search, we create a simple yet powerful RAG system that works reliably at scale.

---

## ğŸ“š Pipeline Sequence

| Order | Stage | Required? | Output |
|-------|-------|-----------|--------|
| 1ï¸âƒ£ | Data Extraction | âœ… Yes | Markdown, Tables, Images |
| 2ï¸âƒ£ | Image Descriptions | âœ… Yes | Text descriptions of images |
| 3ï¸âƒ£ | Data Ingestion | âœ… Yes | Vector database populated |
| 4ï¸âƒ£ | Retrieval | âœ… Yes | Search & retrieval functions |

---

**End of Pipeline Documentation**
